[
{
	"uri": "http://example.org/",
	"title": "NeuralSight Home Page",
	"tags": [],
	"description": "",
	"content": "NeuralSight Documentation Neural Labs Africa is a Health Tech Company creating AI based solutions to solve, augment and revolutionize the pressing challenges in medical diagnostics.\nThis documentation enables you to setup our platform locally.\nDocumentation Summary  Getting Started  Chapter 1 Getting Started NeuralSight Medical Imaging. Neural Labs Africa is a Health Tech Company creating AI based solutions to solve, augment and revolutionize the pressing challenges in medical diagnostics. Impact  Get-to-Understand-NeuralSight-AI  What is in this Documentation? This repository contains a non technical overview of the project NeuralSight. It explain in brief about the solution, the objective and requirements of the project. It also gives an oveview of how to aqquire data necessary to successfully run the project, how to de-identify the data to protect patient information and the process of data annotation. You will get to understand Quality control methods and approaches to data modelling to successfully build deep learning algorithms and performance evaluation that gives you a better view on how to ensure you develop a reliable and accurate Ai algorithm.\n Company Charter  Introduction Neural Labs Africa is an innovative medical technology company screening Medical Images for Radiologists \u0026amp; Hospitals in real-time to identify diseases such as Pneumonia and Tuberculosis. Read the Documentation Online What is in This Documentation? This repository contains Neural Labs Africa Company Chater, Contributor Policies and Guidelines. Table of contents: Vision Structure The problem Value Proposition Project Scope Project Risks Values and Principles Contribution Policies Contributor GuideLines Code of Conduct License Important Resources: What is NeuralSight NeuralSight Project Documentation NeuralSight AI Repository NeuralSight Frontend Repository Have a new Feature or an Issue you want fixed?\n Installation  Item Name: NeuralSight AI Item Version: V 1.0.0 Authors: Paul N. Mwaura \u0026amp; Stephen Kamau Support Forum: https://neurallabs.africa Contact Us: info@neurallabs.africa License: GPL-3 License The following steps are here to help you initialize your new website. If you don\u0026rsquo;t know Hugo at allFirst of all, Thank you so much for your amazing project. You are awesome! You are entitled to contribute and get free code updates to this product + exceptional support from the author directly.\n Contribute to this documentation Feel free to update this content, just click the Edit this page link displayed on top right of each page, and pullrequest it Your modification will be deployed automatically when merged.\n "
},
{
	"uri": "http://example.org/getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Chapter 1 Getting Started NeuralSight Medical Imaging. Neural Labs Africa is a Health Tech Company creating AI based solutions to solve, augment and revolutionize the pressing challenges in medical diagnostics.\nImpact "
},
{
	"uri": "http://example.org/getting-started/nsight/",
	"title": "Get-to-Understand-NeuralSight-AI",
	"tags": [],
	"description": "",
	"content": "What is in this Documentation? This repository contains a non technical overview of the project NeuralSight. It explain in brief about the solution, the objective and requirements of the project. It also gives an oveview of how to aqquire data necessary to successfully run the project, how to de-identify the data to protect patient information and the process of data annotation.\nYou will get to understand Quality control methods and approaches to data modelling to successfully build deep learning algorithms and performance evaluation that gives you a better view on how to ensure you develop a reliable and accurate Ai algorithm.\nLastly, we will take you through the project roadmap and future technology innovations to be implemented by Neural Labs Africa.\nTable of Contents  Introduction The Solution Objective Project Requirements Data Acquisition Data De-identification Data Annotation Quality Control Data Modeling Performance evaluation Software Road Map Future Technology Innovations Obstacles of AI in Medical Imaging  Important Resources:  NeuralSight Project Documentation NeuralSight AI Repository NeuralSight Frontend Repository Company Charter Have a new Feature or an Issue you want fixed?  1. Introduction Radiologists are faced with a high volume of medical images and a shortage of radiologists. Across the world, there\u0026rsquo;s also an exponential increase in the number of patients who need radiology services. Our solution is deep learning and computer vision based technology that screens medical images in real time, helping Radiologists save time, reduce radiation exposure for patients, and improve patient experience.\nWe are proposing web Software as a service (SAAS) and mobile application( to be developed in the future) that will be accessible to medical practitioners such as radiologists, radiographers, hospitals and diagnostic centres. This involves developing an application based on the DICOM standard which allows the upload of medical images such as digital radiography, ultrasonography, secondary pictures and scanned images, digital angiography etc.,in various formats (JPEG,PNG ) be it monochromatic, coloured, static images, uncompressed and compressed images.\n2. The Solution Our AI algorithm NeuralSightâ„¢ is capable of identifying 15 respiratory and heart diseases which include: Pneumonia, COVID-19, Aortic enlargement, Atelectasis, Calcification, Cardiomegaly,Consolidation, Interstitial Lung Disease (ILD), Infiltration, Lung Opacity, Nodule/Mass, Pleural effusion, Pleural thickening, Pneumothorax, Pulmonary fibrosis in Real Time. The ability to quantify the percentage of the lung affected due to lesions enables objective monitoring of disease progression.\nWe have evaluated and identified the problems faced by medical practitioners in Africa. Doctors and clinicians need the following for a successful deployment of AI solution in the clinical environment: Models trained on datasets that represent the population; Seamless integration into clinical workflows; A technology platform to connect research and hospitals. Automatically highlighting areas of pathology and calculating parameters to determine a diagnosis. In a perfect case scenario, algorithms will be able to create a pre-filled medical opinion using a variety of similar cases. In this case, the doctor will either approve or disapprove of the medical statement and thus save time.\n3. Objective NeuralSight is able to localize, qualify the lesions and score in under a minute, enabling clinicians to classify patients into priority categories: high, medium, low and none. The ability to quantify the percentage of the lung affected due to lesions enables objective monitoring of disease progression. Ai-powered program management for tracking end-to-end disease management.\n4. Project Requirements The success of the project development is dependent on key factors that will influence the accuracy, efficiency and reliability of the platform. That said, some of these factors do not have a replacement hence due diligence on the cost, ease of access and value addition have to be placed in consideration by both the management and tech team at Neural Labs Africa.\nThese requirements include;\n Consultant Radiologist Complete Developer team(Frontend Dev/ Full-Stack Dev, Data Scientist, Machine Learning Engineer(MLOps) and UI/UX Designers) Advisory Board, both medical and business Research \u0026amp; Development team  5. Data Acquisition Medical image data are acquired for different purposes, such as diagnosis, therapy planning, intraoperative navigation, post-operative monitoring, and biomedical research. Major requirements that guide the selection of imaging modalities in practice include: the relevant anatomy must be depicted completely, the resolution of the data should be sufficient to answer specific diagnostic and therapeutic questions, the image quality with respect to contrast, signal-to-noise ratio (SNR) and artifacts must be sufficient to interpret the data with respect to diagnostic and therapeutic questions, exposure and burden to the patient and to the medical doctor should be minimized, and costs should be limited.\n6. Data De-identification According to the U.S. Health Insurance Portability and Accountability Act, or HIPAA, and the European General Data Protection Regulation, both retrospectively and prospectively gathered data require proper de-identification. Sensitive information includes but is not limited to name, medical record number, and date of birth. Identifiable information is commonly available in DICOM metadata(header) and multiple tools are available to automatically remove this information. When radiology data is shared in open-source research efforts, the DICOM metadata is often removed completely or converted to another format such as Neuroimaging Informatics Technology Initiative, or NIFTI, which retains only voxel size and patient position. Totally removing the DICOM metadata for open-source research efforts prevents privacy issues but reduces the value of data, because metadata is important for AI algorithm development.\n7. Data Annotation Medical image annotation is the process of labeling medical imaging data such as X-Ray, CT, MRI scans, Mammography, or Ultrasound. It is used to train AI algorithms for medical image analysis and diagnostics, helping doctors save time, make better-informed decisions, and improve patient outcomes.\nImage labels are annotations performed by medical experts such as radiologists. These annotations can be considered ground truth if imaging is the reference standard (eg, pneumothorax). Choosing the appropriate label for a given imaging AI application requires a balance between finding the best discriminating categories (ie, normal vs emergent) and clinically relevant granularity (ie, subtype of liver lesion) depending on the desired task. With the exception of AI methods that enhance image quality, medical images in isolation are generally not suitable for developing diagnostic AI models unless associated with a diagnosis through the free-text radiology report (which require additional labeling strategies discussed below), expert consensus, segmentation, or an applied ground truth label such as electronic phenotyping.\n8. Quality Control Medical Quality Control (QC) refers to the specific test required to ensure effective and safe equipment performance. QC tests check the performance of the equipment under routine clinical conditions, following established protocols for facilities, equipment and procedures. Quality Assurance involves planned and systematic actions that will produce consistently high quality images with minimum exposure of the patients and workers. Quality Assurance actions include both \u0026ldquo;Quality Control Techniques\u0026rdquo; and \u0026ldquo;Quality administration procedures\u0026rdquo;.\n \u0026ldquo;Quality Assurance Program\u0026rdquo; means an organized entity designed to provide quality assurance for a Radiology facility. \u0026ldquo;Quality Control Techniques\u0026rdquo; are those techniques used in the monitoring, testing, and maintenance of the components of radiology equipment. \u0026ldquo;Quality Control Procedures\u0026rdquo; are the procedures that provide the organizational framework for the \u0026ldquo;Quality Assurance Program\u0026rdquo;. They are the actions that guarantee that the monitoring techniques are uniformly performed and evaluated and that necessary  Quality Assurance Program in radiology facility is determined by an analysis of the facility\u0026rsquo;s objectives and resources and should include the following major constituents:\n Responsibilities Purchase specifications Standards for Image Quality Monitoring and Maintenance programs Installation/Operational/ Performance qualifications of equipment Records Quality Assurance Manual Training  9. Data Modeling NeuralSight is implemented using federated learning architecture.\n10. Performance evaluation The proper functional performance of the AI solution in pathology is also assessed in regulatory procedures. In the EU, the performance evaluation required has three aspects: Scientific validity: Shows valid association between the software output and the targeted clinical condition. Analytical performance: Shows that the software produces accurate and reliable output. Clinical performance: Shows that the output meets the intended purpose in a clinical context and for the target population.\nDetection of different findings on chest x rays Annotation of the findings.(Drawing bounding boxes). Classify patients into priority categories: high, medium, low and none. Flag abnormal scans\n11. Software Road Map Research Prototype  Defined task and found applicable algorithmic approach. Tested feasibility of algorithm on first data. Evaluated performance of algorithm on limited dataset.  Clinical Prototype  Evaluated algorithm on real-world data from routine pathology. Integrated solution prototype usable by developers. Prototype evaluated in routine pathology.  Product  Documented test results and performance assessment for regulatory approval. Solution in production use in routine pathology.   Integration AI solutions in pathology are usually standalone tools, each having its own interface for data input, output and execution. To ensure flawless usability in diagnostic routine, these tools must be integrated into the laboratory IT infrastructure and made interoperable with software developed by other manufaturers.\nTo use AI solutions in pathology you would require three pieces of software.\n Image Archive Lab Information System Pathology Workstation  Integration of artificial intelligence Integration of AI solutions into the laboratory IT infrastructure can be done as either standalone tools or workstation extensions. One huge advantage of standalone tools is their flexibility. This means they can be tailored to the specific needs of the analysis methods in terms of user interaction, parameterization and visualization. Workstation extensions, on the other hand, have the major advantage of workflow efficiency. They can be used without having to leave familiar workstation environments and without having to learn new tools. They also have a low development cost and are very interoperable software from other manufacturers.\nCooperation is crucial It is important for Neural Labs Africa to involve pathologists from an early stage of development. To obtain sufficient amounts of training and test data, partnerships must be established with pathologists and incentives be provided for them to provide image data and required annotations. Partnerships with multiple laboratories is essential in order to obtain a sample of interlaboratory variability.\nOther challenges There are further challenges in producing AI solutions that are beyond the scope of this report. This concerns the many algorithmic challenges associated with AI development, which have already been addressed in numerous publications. Other concerns include issues of usability and user experience.\n12. Future Technology Innovations Neural Labs Africa has a bright vision to empower doctors and partners in the medical field with the aim of enabling access to diagnostic healthcare for all in Africa. We are looking into researching in different sectors using AI for Clinical Decision Support System (Ð¡DSS). Some of these areas include:\n AI-based software that enables doctors to calculate parameters for surgeries automatically, which allows them to avoid miscalculations and transfer the patient to the surgery department faster. AI-based analytics for screenings, precision medicine, and risk assessment to help doctors look at the genetics, environment, and lifestyle of a person in order to select a treatment that could work best for them. Monitoring nursesâ€™ and doctorsâ€™ actions can help maintain patient bypass schedules. AI-driven video analytics can detect whether or not the patient was given the right medicine at the right time.  Who is to Benefit? Healthcare is one of the few areas where even the slightest mistake can be critical. Both healthy people and hospital patients can receive an accurate doctorâ€™s diagnosis, prescriptions, and recommendations.\nDoctors are, first of all, people, and people can be distracted or tired. While everything depends on their decision, we need to develop more automated diagnostics solutions to help them identify conditions quicker, promoting early intervention.\nResearchers have to go through vast amounts of data daily when discovering new drugs, conducting genetic research, or clinical trials. With AI-driven technology in medical imaging, they can process even larger data sets, approve new-to-market medications faster, and drive advances in modern healthcare.\nHealthcare institutions can take advantage of computer vision technologies to manage operational processes. A few application examples would be monitoring staff activities or online check-in terminals.\nGovernments can predict the spread of viruses and diseases by analyzing large amounts of data.\n13. Obstacles of AI in Medical Imaging   Decentralized storage   Medical institutions usually store patient data on their local systems. The problem is that there is no centralized storage for the entire medical history of all patients. By analyzing big data individually and in combination, we can improve and control the quality of diagnostics and predict the course of diseases.\n  General Data Protection Regulations Compliance   General Data Protection Regulations do not clearly explain the relationship between the developer and the medical personnel. The lack of transparent legal mechanisms delays the broader adoption of AI in medical imaging. Another technical issue is the time-consuming legalization of computer vision and AI-based solutions. The product must be registered and undergo various tests before being used in medical practice.\n  Difficulties in Organizing Expert Consensus   It takes more than one doctor to train neural networks while developing a CDSS (Clinical Decision Support System) solution. Five or more specialists must verify the results before making a solid conclusion to avoid misinterpretations and mistakes that can be critical.\n  Conservatism   The medical communityâ€™s reluctance to embrace a new product is normal. The only thing that can help overcome the issue is the increasing implementation of successful use cases confirming the effectiveness of innovative solutions.\n Frankly, AI isnâ€™t going to replace clinicians, it isnâ€™t going to remove enormous amounts of workload and it certainly isnâ€™t going to autonomously make treatment decisions. Those arenâ€™t the problems that need to be solved right now and AI in healthcare is not going to be given the opportunity to run before it can walk (nor should it).\n "
},
{
	"uri": "http://example.org/getting-started/charter/",
	"title": "Company Charter",
	"tags": [],
	"description": "",
	"content": "Introduction Neural Labs Africa is an innovative medical technology company screening Medical Images for Radiologists \u0026amp; Hospitals in real-time to identify diseases such as Pneumonia and Tuberculosis. Read the Documentation Online\nWhat is in This Documentation? This repository contains Neural Labs Africa Company Chater, Contributor Policies and Guidelines.\nTable of contents:  Vision Structure The problem Value Proposition Project Scope Project Risks Values and Principles Contribution Policies Contributor GuideLines Code of Conduct License  Important Resources:  What is NeuralSight NeuralSight Project Documentation NeuralSight AI Repository NeuralSight Frontend Repository Have a new Feature or an Issue you want fixed?  1. Vision To positively impact healthcare by using AI, a revolutionary technology to democratize access to diagnostic healthcare.\n2. Structure Neural Labs Africa is founded by two passionate Machine Learning Engineers towards solving Africa\u0026rsquo;s most pressing challenges using technology.\n3. The Problem In Africa, healthcare services are overused yet under-resourced challenging healthcare providers with a significant burden to diseases. A patientâ€™s wait time to receive analyzed lab results is often more than 2 weeks. This has resulted in late treatment reducing the chances of survival especially with fast killing diseases such as Tuberculosis, Pneumonia and COVID-19.\n4. Value Proposition Using Deep Learning and Computer Vision our technology screens x-rays for radiologists and hospitals in real time to identify over 13 respiratory diseases such as Pneumonia, TB, and lung cancer. Now, instead of waiting for two weeks, we empower doctors to provide accurate diagnoses before patients even get their shirts back on.\n5. Project Scope Neural Labs Africa is carrying out its operations in West Africa focusing on Francophone Africa. Such plans to upscale our technology in different African regions is fostered by our vision to build a solution for Africa, a region where diseases are overwhelming its population. Using our current MVP to run clinical trials of our Software where we are engaging end-users to provide tailor made predictive models within the diverse medical imaging modalities. Moreover, Neural Labs Africa is focusing on strategic partnerships with hospitals, Tele-Radiology Laboratories, and specialists to help impact and improve healthcare delivery within the different institutions.\n6. Project Risks Access to Data is among the most pressing challenges we are facing. This is because different countries have different data regulations. For example, in Kenya Data collected within the Kenyan demographic region should be stored within the Kenyan Demographic region as a result most hospitals and institutions prefer to keep their data offline to meet these thresholds hence such data is not accessible on cloud.\n7. Values and Principles The solutions that Neural Labs Africa creates for clients are only as excellent as the people who create them. Neural Labs Africa conducts cultural experiments and actively involves its staff in evolving the company\u0026rsquo;s culture. The culture essentially combines agile administration, independence, self-governance, routine evaluations, and participation monitoring. This strategy has influenced mentoring and growth in the future. The transparency of Neural Labs Africa is something we take great delight in. The company has shared the project repository on open source platforms for the open source.\n8. Contribution Policy You are welcome to submit an update in the repo NeuralSight frontend if you do have any ideas that could enhance NeuralSight. If you have any code that enhances a component of NeuralSight or implements new features, please submit a pull request to the associated repository. If those portions of code are tested and documented, it is more than welcome.\n9. Contributor GuideLines This guide explains how to contribute to the NeuralSight project. It defines working practices of the development team. This document helps new contributors start working on the project. It is a living document and will change. If you think something could be better, please open an issue with your feedback.\nSet up a development environment Contents:\n Requirements Open Pull Request Discord Maintainer Response Time  Requirements To setup NeuralSight development environment, you need the following:\n Python3 installed Discord Account  Open a pull request These guidelines help maintainers review new pull requests. Stick to the guidelines for quicker and easier pull request reviews.\n    We prefer gradual, small changes over sudden, big changes Write a helpful title for your pull request (if someone reads only one sentence, will they understand your change?) Address the following questions in your pull request:\n What is a short summary of your change? Why is this change helpful? Any specific details to consider? What is the desired outcome of your change?  Join Our Discord Channel To better engahge with the development team, we would recommend you to sign up on Discord if you do yet have an account, then join our channel Neural Labs Africa Discord channel.\nMaintainer response time Project maintainers make a best effort to respond in 10 days or less to new issues. Current maintainers are volunteers working on the project, so we try to keep up as best we can. If more than 10 days passed and you have not received a reply, follow up on Neural Labs Africa Discord channel. Someone may have missed your comment â€“ we are not intentionally ignoring anyone.\nRemember, using issue templates and answering the above questions in new pull requests reduces the response time from a maintainer to your issue or pull request.\n10. Code of Conduct This project adheres to a Code of Conduct based on Python. As a community member you have to read and agree with it.\nFor more information please contact us and/or visit the original Python Code of Conduct homepage.\nLicense The code in this repository is licensed under the GPL 3.0 license. This license allows for free use, distribution, and modification of the code, with the requirement that any derivative works must also be released under the GPL 3.0 license. You can find the full text of the GPL 3.0 license at the following link: https://www.gnu.org/licenses/gpl-3.0.en.html and a summary of the license can be found here: https://www.gnu.org/licenses/quick-guide-gplv3.html.\nPlease note that while this code is open-sourced, any medical data used in this code should be used only with permission and under strict compliance with patient privacy laws and regulations.\nAdditional Information Please reach out to us at info@neurallabs.africa\n"
},
{
	"uri": "http://example.org/getting-started/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "  Item Name: NeuralSight AI Item Version: V 1.0.0 Authors: Paul N. Mwaura \u0026amp; Stephen Kamau Support Forum: https://neurallabs.africa Contact Us: info@neurallabs.africa License: GPL-3 License   The following steps are here to help you initialize your new website. If you don\u0026rsquo;t know Hugo at allFirst of all, Thank you so much for your amazing project. You are awesome! You are entitled to contribute and get free code updates to this product + exceptional support from the author directly. You will learn more about NeuralSight AI by following this Neural Labs Africa.\nTable of contents:  Getting Started NeuralSight AI Intro Set Up Locally Install Dependencies Generating CSV File Data Pre_Processing for YOLO Data Modelling Copyright and License  Introduction First of all, Thank you so much for your amazing project. You are awesome! You are entitled to contribute and get free code updates to this product + exceptional support from the author directly.\nThis documentation is to help you regarding each step of setting up and running it locally. Please go through the documentation carefully to understand how to setup properly. Python knowledge is required to setup and contribute to this project. You may learn basics of Python Documentation here and Python Tutorial here.\nRequirements You will need the following softwares to get started.\nCode Editing Software (eg: Visual Studio Code, Pycharm Community version or Notepad) Google Colab for free access to cloud GPU\nBe careful while installing dependencies. If not installed properly, the code may break completely. We would recommend setting up a virtual environment. No support is provided for older libraries not included in the dependencies.\n Getting Started Easy to Use NeuralSight is an Intelligent tool that offers a great opportunity to enhance and augment radiology services thereby relieving the bottleneck in medical imaging diagnosis. NeuralSight provides a high-level web-interface equipped with advanced annotation tools and project management features. NeuralSight was designed from the ground up to enhance the radiology workflow and reduce backlogs in the workflow.\nFocus on What Matters NeuralSightâ„¢ provides automated interpretation of radiology exams like CXRs, CTs and MRI scans to identify pathologies and diseases such as Pneumothorax, Cardiomegaly, Benign breast Tumour, Malignant breast Cancer, Atelectasis, Infiltration, Emphysema, Mass/Nodule, Pleural Thickening, Effusion, Consolidation, Tuberculosis, Pneumonia, Prostate and Lung cancer.\nOur Impact To provide a thorough and actionable innovation artificial intelligence solutions that will help the healthcare sector achieve its objectives. reduces workload and thereby eases such costs and provides a friendly pricing model to patients. Offering affordable health care services will ensure that everyone, even those in marginalised communities, are able to receive healthcare services. AI to solve the overwhelming UN Strategic Development Goal 3 Good Health and Wellbeing has the potential to ensure that Africa improves its healthcare system.\nThe Tool NEURALSIGHT system is a combination of YOLOv5 trained model and a set of endpoints that allows users to detect 14 Chest X-ray pathologies. The model is trained using a VinBig dataset of X-ray images and annotations of the 14 pathologies of interest. The model is able to predict bounding boxes and class probabilities for each object (pathology) in an image. The endpoints allow users to perform tasks such as logging in and uploading images for prediction, as well as viewing and downloading the prediction results and reports. The model is already trained hence the users only require to upload their XRAY images and be able to get response as the prediction and reports. The model has been evaluated on a test dataset and has shown to have good performance in terms of metrics such as average precision and mean average precision. The model is deployed using FastAPI, which is a lightweight web framework for building web applications and APIs, and is easily integrated with other technologies.\nData Dataset Description For this dataset, the main aim was to come up with an object classifying system for common thoracic lung diseases and localizing critical findings. This is an object detection and classification problem. The images are in png file with difference dimensions i.e 1024, 216 and 512. In this project, Images that were used were the one with 512 dimensions.\nDataset information The dataset comprises 67914 annotated and those not annotated images where there was only 4394 unique patients images that were annotated for posterior-anterior (PA) Xrays scans in PNG format. All images were labeled by a panel of experienced radiologists for the presence of 14 critical radiographic findings as listed below:\nPathologies: 0 - Aortic enlargement 1 - Atelectasis 2 - Calcification 3 - Cardiomegaly 4 - Consolidation 5 - ILD 6 - Infiltration 7 - Lung Opacity 8 - Nodule/Mass 9 - Other lesion 10 - Pleural effusion 11 - Pleural thickening 12 - Pneumothorax 13 - Pulmonary fibrosis The \u0026quot;No finding\u0026quot; observation (14) was intended to capture the absence of all findings above but in this case it was ignored. Data preprocessing The Model (YOLOv5) requires specific data preprocessing steps in order to work effectively. In this case, we were to process our information to meets this standard. The following is a detailed workflow documentation of the data preprocessing steps that were taken in this particular case: The main difference between GroupKFold and other cross-validation techniques like train_test_split is that it helped takes into account pathologies or labels for each class in the sample split equally. i.e, Since some pathologies had more class Distribution than others, It helped us ensure that the samples from one class label of pathology do not end up in the same group and are equally Distributed in both groups. For this project, since the data is split into 5 folds, 1/5 (or 20%) of the data will be used as the test set, while 4/5 (or 80%) will be used as the training set.\nPreparation of dataset Structure for modelling. To structure the images and labels for YOLOv5 training, We created the following 4 directories where the training and validation information was to reside.\n1. `/WORKING_DIR/PROJECT_DIR/labels/train` - This directory will store the labels (annotations) for the training images. 2. `/WORKING_DIR/PROJECT_DIR/labels/val` - This directory will store the labels (annotations) for the validation images. 3. `/WORKING_DIR/PROJECT_DIR/images/train` - This directory will store the training images. 4. `/WORKING_DIR/PROJECT_DIR/images/val` - This directory will store the validation images. Once the directories were created, We then moved or copy the images and their corresponding labels to the appropriate directories. The labels for each image should have the same file name as the image, and should be placed in the appropriate label directory\n(either `/WORKING_DIR/PROJECT_DIR/labels/train` or `/WORKING_DIR/PROJECT_DIR/labels/val`). The labels were also in their own format. Each image had a single text file as labels. The format of labels for YOLOv5 model used was a plain text file with the same name as the corresponding image file, and with a .txt extension. The file should contain one line for each object in the image, with the following format:\nNeuralSight Intro Let\u0026rsquo;s discover NeuralSight in less than 5 minutes.\nGetting Started Get started by setting up your environment. Or try our demo with neurallabs.africa. Please read more about NeuralSight here. NeuralSight Github Repository.\n What you\u0026rsquo;ll need Python version 3.5 or above:\n When installing Python, you are recommended to check all checkboxes related to dependencies.  Set Up Locally Clone the repository to your local machine.\ngit clone https://github.com/NeuralSight/NeuralSight_AI.git You can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.\ncd NeuralSight_AI Install Dependencies The cd command changes the directory you\u0026rsquo;re working with. In order to work with your newly created Docusaurus site, you\u0026rsquo;ll need to navigate the terminal there. Install all the dependencies using the requirements.txt file.\npip install -r requirements.txt The command installs all necessary dependencies you need to run NeuralSight.\nGenerating CSV File Add Medical files to the workspace for preprocessing:\npython preprocess.py Create your final preprocessed csv dataset - Create a file at NeuralSight_AI/training/process.py:\n#import some modules from tqdm.notebook import tqdm from glob import glob import shutil, os import cv2 import pandas as pd import numpy as np from cleaning import create_data import warnings warnings.filterwarnings(\u0026quot;ignore\u0026quot;); import dotenv dotenv.load_dotenv() DATA_DIR = os.getenv(\u0026quot;DATA_DIRECTORY\u0026quot;) TRAIN_DIR = os.getenv(\u0026quot;TRAIN_DIR\u0026quot;) WANDB_API_KEY = os.getenv(\u0026quot;WANDB_API_KEY\u0026quot;) #check if the file exists if not os.path.isfile(f'{DATA_DIR}/train.csv'): print(f\u0026quot;No SUch Directory in the path Passed : PATH SEND :: {DATA_DIR}/train.csv\u0026quot;) print(\u0026quot;Exiting....\u0026quot;) exit(0) # read the data df = pd.read_csv(f'{DATA_DIR}/train.csv') df1 = df[[\u0026quot;image_id\u0026quot;,\u0026quot;x_min\u0026quot;,\u0026quot;y_min\u0026quot;,\u0026quot;x_max\u0026quot;,\u0026quot;y_max\u0026quot;,\u0026quot;class_id\u0026quot;, \u0026quot;height\u0026quot;, \u0026quot;width\u0026quot;, \u0026quot;class_name\u0026quot;]].dropna() df1['path'] = df1['image_id'].apply(lambda x: f\u0026quot;{DATA_DIR}/train/{x}.png\u0026quot;) # get class id class_dict = {v:k for k,v in dict(df1[['class_name', \u0026quot;class_id\u0026quot;]].values).items()} # save df1 in order to use it for plotting df1.to_csv(\u0026quot;train_clean.csv\u0026quot;, index=False) check_df = create_data(df1) # class names available classes = list(check_df['class_name'].unique()) # Yolo normaly requires bbox to be normalized between 0 and 1. # Since we have have height and width for a particular image, we gonna just divide bbox against either widht or height. # We then extract the widht and height of the bbox from the above calibrations df = check_df.copy() df['x_min'] = df.apply(lambda x: (x.x_min)/x.width, axis =1) df['y_min'] = df.apply(lambda x: (x.y_min)/x.height, axis =1) df['x_max'] = df.apply(lambda x: (x.x_max)/x.width, axis =1) df['y_max'] = df.apply(lambda x: (x.y_max)/x.height, axis =1) df['x_mid'] = df.apply(lambda x: (x.x_max+x.x_min)/2, axis =1) df['y_mid'] = df.apply(lambda x: (x.y_max+x.y_min)/2, axis =1) df['w'] = df.apply(lambda x: (x.x_max-x.x_min), axis =1) df['h'] = df.apply(lambda x: (x.y_max-x.y_min), axis =1) df['area'] = df['w']*df['h'] selected_classes = (df['class_name'].value_counts(normalize=True).T[df['class_name'].value_counts(normalize=True).T \u0026gt;0.0001]) SELECTED_CLASS_NAMES = list(selected_classes.index) # Get a dataframe with selected classes selected_df = df[df['class_name'].isin(SELECTED_CLASS_NAMES)] # only which have below 40 bbox instances img_ids = list((df['image_id'].value_counts(normalize=False).T[df['image_id'].value_counts(normalize=False).T \u0026lt;=40]).index) # pd.DataFrame(base_details) TRAIN =[] # for img_id in selected_df['image_id'].unique(): for img_id in selected_df[selected_df['image_id'].isin(img_ids)]['image_id'].unique(): curr_df = selected_df[selected_df['image_id'] ==img_id].reset_index(drop=True) base_details = dict(curr_df.loc[0][['image_id',\u0026quot;path\u0026quot;,'width', 'height']]) information =[] for indx in range(curr_df.shape[0]): other_details = dict(curr_df.loc[indx][['class_name', \u0026quot;x_min\u0026quot;, \u0026quot;y_min\u0026quot;,\u0026quot;x_max\u0026quot;,\u0026quot;y_max\u0026quot;, \u0026quot;x_mid\u0026quot;, \u0026quot;y_mid\u0026quot;, \u0026quot;w\u0026quot;, \u0026quot;h\u0026quot;, \u0026quot;area\u0026quot; ]]) information.append(other_details) TRAIN.append([base_details['image_id'],base_details['path'] ,base_details['width'],base_details['height'],information]) final_data = pd.DataFrame(TRAIN, columns =['image_id',\u0026quot;path\u0026quot;, \u0026quot;width\u0026quot;, \u0026quot;height\u0026quot;, \u0026quot;information\u0026quot;]) from pprint import pprint print(final_data.head()) # save this data for feature usage. final_data.to_csv(\u0026quot;processed.csv\u0026quot;, index=False) Data Pre_Processing for YOLO Add Medical files to the workspace for preprocessing:\npython preprocess.py Data Pre_Processing for YOLO - Create a file at NeuralSight_AI/training/process.py:\n Dropping images with No findings label: The first step in preprocessing the data was to drop all images with their label information that had a label called \u0026ldquo;No findings\u0026rdquo; because the label had no any bounding box information which were needed for model training. This was done to ensure that the model only focuses on images that contain the other 13 Chest X-ray pathologies of interest. Removing overlapping bounding boxes which occurs in the same class using Non-maximum suppression methods: In order to improve the performance of the model, non-maximum suppression was used to remove overlapping bounding boxes which occurs in the same class label. This technique helps to reduce the number of false positives by removing bounding boxes that have a high overlap with other boxes of the same class. The method worked as follows. Non-maximum suppression(NMS) function is used for removing overlapping bounding boxes. The function takes in two inputs: \u0026ldquo;boxes\u0026rdquo; and \u0026ldquo;overlapThresh\u0026rdquo;. The \u0026ldquo;boxes\u0026rdquo; is a list of bounding boxes in the format of (x1, y1, x2, y2) where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner. \u0026ldquo;overlapThresh\u0026rdquo; is a threshold value for the overlap ratio of bounding boxes. The function first checks if the input \u0026ldquo;boxes\u0026rdquo; is an empty list. If it is, it returns an empty list. If there are boxes, the function extracts the x and y coordinates of the top-left corner and bottom-right corner of each box. It then computes the area of each bounding box and sorts the boxes based on the bottom-right y-coordinate. Next, the function iterates through each box and compares it with the other boxes to calculate the ratio of overlap. If the overlap ratio is greater than the threshold, the function removes that box from the list of boxes to be returned. Finally, the function returns the remaining boxes as integers. Bounding Box information Normalization: The bounding box information were normalized to standard values between 0 and 1 against the width and height of the image. This ensures that the bounding box coordinates are consistent regardless of the size of the input image that a user will feed to the model. It allows us to work on any image size available. Calculating the height and width of the bounding box since they are one of the crucial features that must be fed to the model during training, they are used to identify where what is the size of the box. Training and Validation. Before Training, We split the data input two in order to validate the model after training. We used GroupKFold method to split. This method is a variation of the KFold method for cross-validation. It is used to split the data into k number of folds, where k in this case, we used k= 5 which means the data will be split into 5 folds.  from tqdm.notebook import tqdm from glob import glob import shutil, os import cv2 import pandas as pd import numpy as np from cleaning import create_data import warnings warnings.filterwarnings(\u0026quot;ignore\u0026quot;); import dotenv dotenv.load_dotenv() DATA_DIR = os.getenv(\u0026quot;DATA_DIRECTORY\u0026quot;) TRAIN_DIR = os.getenv(\u0026quot;TRAIN_DIR\u0026quot;) WANDB_API_KEY = os.getenv(\u0026quot;WANDB_API_KEY\u0026quot;) #check if the file exists if not os.path.isfile(f'{DATA_DIR}/train.csv'): print(f\u0026quot;No SUch Directory in the path Passed : PATH SEND :: {DATA_DIR}/train.csv\u0026quot;) print(\u0026quot;Exiting....\u0026quot;) exit(0) # read the data df = pd.read_csv(f'{DATA_DIR}/train.csv') df1 = df[[\u0026quot;image_id\u0026quot;,\u0026quot;x_min\u0026quot;,\u0026quot;y_min\u0026quot;,\u0026quot;x_max\u0026quot;,\u0026quot;y_max\u0026quot;,\u0026quot;class_id\u0026quot;, \u0026quot;height\u0026quot;, \u0026quot;width\u0026quot;, \u0026quot;class_name\u0026quot;]].dropna() df1['path'] = df1['image_id'].apply(lambda x: f\u0026quot;{DATA_DIR}/train/{x}.png\u0026quot;) # get class id class_dict = {v:k for k,v in dict(df1[['class_name', \u0026quot;class_id\u0026quot;]].values).items()} # save df1 in order to use it for plotting df1.to_csv(\u0026quot;train_clean.csv\u0026quot;, index=False) check_df = create_data(df1) # class names available classes = list(check_df['class_name'].unique()) # Yolo normaly requires bbox to be normalized between 0 and 1. # Since we have have height and width for a particular image, we gonna just divide bbox against either widht or height. # We then extract the widht and height of the bbox from the above calibrations df = check_df.copy() df['x_min'] = df.apply(lambda x: (x.x_min)/x.width, axis =1) df['y_min'] = df.apply(lambda x: (x.y_min)/x.height, axis =1) df['x_max'] = df.apply(lambda x: (x.x_max)/x.width, axis =1) df['y_max'] = df.apply(lambda x: (x.y_max)/x.height, axis =1) df['x_mid'] = df.apply(lambda x: (x.x_max+x.x_min)/2, axis =1) df['y_mid'] = df.apply(lambda x: (x.y_max+x.y_min)/2, axis =1) df['w'] = df.apply(lambda x: (x.x_max-x.x_min), axis =1) df['h'] = df.apply(lambda x: (x.y_max-x.y_min), axis =1) df['area'] = df['w']*df['h'] selected_classes = (df['class_name'].value_counts(normalize=True).T[df['class_name'].value_counts(normalize=True).T \u0026gt;0.0001]) SELECTED_CLASS_NAMES = list(selected_classes.index) # Get a dataframe with selected classes selected_df = df[df['class_name'].isin(SELECTED_CLASS_NAMES)] # only which have below 40 bbox instances img_ids = list((df['image_id'].value_counts(normalize=False).T[df['image_id'].value_counts(normalize=False).T \u0026lt;=40]).index) # pd.DataFrame(base_details) TRAIN =[] # for img_id in selected_df['image_id'].unique(): for img_id in selected_df[selected_df['image_id'].isin(img_ids)]['image_id'].unique(): curr_df = selected_df[selected_df['image_id'] ==img_id].reset_index(drop=True) base_details = dict(curr_df.loc[0][['image_id',\u0026quot;path\u0026quot;,'width', 'height']]) information =[] for indx in range(curr_df.shape[0]): other_details = dict(curr_df.loc[indx][['class_name', \u0026quot;x_min\u0026quot;, \u0026quot;y_min\u0026quot;,\u0026quot;x_max\u0026quot;,\u0026quot;y_max\u0026quot;, \u0026quot;x_mid\u0026quot;, \u0026quot;y_mid\u0026quot;, \u0026quot;w\u0026quot;, \u0026quot;h\u0026quot;, \u0026quot;area\u0026quot; ]]) information.append(other_details) TRAIN.append([base_details['image_id'],base_details['path'] ,base_details['width'],base_details['height'],information]) final_data = pd.DataFrame(TRAIN, columns =['image_id',\u0026quot;path\u0026quot;, \u0026quot;width\u0026quot;, \u0026quot;height\u0026quot;, \u0026quot;information\u0026quot;]) from pprint import pprint print(final_data.head()) # save this data for feature usage. final_data.to_csv(\u0026quot;processed.csv\u0026quot;, index=False) Preparation of dataset Structure for modelling - To structure the images and labels for YOLOv5 training, We created the following 4 directories where the training and validation information was to reside.\n1. `/WORKING_DIR/PROJECT_DIR/labels/train` - This directory will store the labels (annotations) for the training images. 2. `/WORKING_DIR/PROJECT_DIR/labels/val` - This directory will store the labels (annotations) for the validation images. 3. `/WORKING_DIR/PROJECT_DIR/images/train` - This directory will store the training images. 4. `/WORKING_DIR/PROJECT_DIR/images/val` - This directory will store the validation images. Once the directories were created, We then moved or copy the images and their corresponding labels to the appropriate directories. The labels for each image should have the same file name as the image, and should be placed in the appropriate labels directory (either /WORKING_DIR/PROJECT_DIR/labels/train or /WORKING_DIR/PROJECT_DIR/labels/val). The labels were also in their own format. Each image had a single text file as labels. The format of labels for YOLOv5 model used was a plain text file with the same name as the corresponding image file, and with a .txt extension. The file should contain one line for each object in the image, with the following format:\nclass_id x_center y_center width height where:\nclass_id: An integer representing the class of the object. This should correspond to the class labels in your dataset. x_center: The x-coordinate of the center of the bounding box, as a fraction of the image width. y_center: The y-coordinate of the center of the bounding box, as a fraction of the image height. width: The width of the bounding box, as a fraction of the image width. height: The height of the bounding box, as a fraction of the image height.\n  Each object should be split in a new line. Here\u0026rsquo;s an example of a label file for an image with two objects, a Pneumothorax and a Cardiomegaly:\n  12 0.5 0.5 0.2 0.2 3 0.7 0.7 0.1 0.1 This means that there is one Pneumothorax object with class label 12, center coordinates (0.5,0.5), width and height as 0.2 and one Cardiomegaly object with class label 3, center coordinates (0.7,0.7), width and height as 0.1\nData Information Pointer - A YAML file was created which contained all information about the training and validation dataset. The file had the following information.\nyaml names: - Cardiomegaly - Pleural effusion - Pleural thickening - Aortic enlargement - Pulmonary fibrosis - ILD - Nodule/Mass - Other lesion - Lung Opacity - Infiltration - Consolidation - Calcification - Atelectasis - Pneumothorax **nc:** 14 **train:** /kaggle/working/train.txt **val:** /kaggle/working/val.txt where:\n train and val is a text file with all directories for images used for training and validation respectively nc is the number of classes names contains all class names  Support Desk Project support will be updated in due course. Please be patient.\nPlease visit our profile page or ask question info@neurallabs.africa\nSupport for the project includes:\n Responding to questions or problems regarding the the project as a whole and its features Fixing bugs and reported issues Providing updates to ensure compatibility with new software versions  Item support does not include:\n Customization and installation services Support for third party software and plug-ins  Before seeking support, please\u0026hellip;\n Make sure your question is a valid Theme Issue and not a customization request. Make sure you have read through the documentation and any related video guides before asking support on how to accomplish a task. Make sure to double-check the theme FAQs. Almost 80% of the time we find that the solution to people\u0026rsquo;s issues can be solved with a simple \u0026ldquo;Google Search\u0026rdquo;. You might want to try that before seeking support. You might be able to fix the issue yourself much quicker than we can respond to your request.  Version History (Changelog) You can find the version history (changelog.txt) file on the main repository or you can contact out support team for assistance page.\nOnce again, thank you so much for purchasing this theme. As I said at the beginning, I\u0026rsquo;d be glad to help you if you have any questions relating to this theme. No guarantees, but I\u0026rsquo;ll do my best to assist. If you have a more general question relating to the themes on ThemeForest, you might consider visiting the forums and asking your question in the \u0026ldquo;Item Discussion\u0026rdquo; section.\nChangelog ----------------------------------------------------------------------------------------- Version 1.0 - Dec 7th, 2023 ----------------------------------------------------------------------------------------- - Updated version as a result of clinical trials. ----------------------------------------------------------------------------------------- Version 0.1 - May 7th, 2022 ----------------------------------------------------------------------------------------- Pilot project version. Copyright and license Code released under the GPL 3.0 License.\nFor more information about copyright and license check Open Source License.\n "
},
{
	"uri": "http://example.org/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]